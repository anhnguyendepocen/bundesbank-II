---
title: "Big Data Analysis"
subtitle: "ML Basics and CART"
author: "Christoph Kern"
output: html_notebook
---

## Setup

```{r}
library(rpart)
library(partykit)
library(caret)
library(pROC)
```

In this exercise, we use a reduced version of the Bank for the Accounts of Companies Harmonized (BACH) data set. "The Bank for the Accounts of Companies Harmonized (BACH) is a free-of-charge database containing the aggregated accounting data of non-financial incorporated enterprises for, so far, 11 European countries." More information about the BACH data can be found here:

https://www.bach.banque-france.fr/?lang=en

```{r}
load("BACH.Rda")
```

## Data preparation

As a starting point, we begin with summarizing all variables to get a first impression of the data.

```{r}
summary(bach)
```

Our machine learning task is to predict whether a corporation experiences a net loss in the year 2015. For this setting, we first compute a binary variable which indicates a loss with "loss" and returns "no_loss" otherwise, based on the variable `bach$net_profit_or_loss`.

```{r}
bach$D_loss <- ifelse(bach$net_profit_or_loss < 0, "loss", "no_loss")
bach$D_loss <- as.factor(bach$D_loss)
summary(bach$D_loss)
```

Then we split the data set into a training and test part. Since the data is structured by time, we want to use the year 2015 for the test set and all remaining years for training.

```{r}


```

## CART

### Grow and prune tree

In order to build a classification tree with the training data, the `rpart()` function can be used. As the outcome variable, we plug in the new variable we created earlier. If this variable is of class factor, `rpart` adapts to this format and grows a classification tree (instead of a regression tree). Use all variables besides `net_profit_or_loss` and `return_on_equity` as predictors, e.g. via `outcome ~ . - net_profit_or_loss - return_on_equity`.

```{r}
set.seed(6342)

```

Next, we want to investigate the Cross-Validation results to find the optimal subtree based on the `rpart` object.

```{r}


```

For pruning, we need the `cp` value of the best subtree. In this code block we prepare tree pruning, e.g. according to the 1-Standard Error rule.

```{r}


```

Now prune the classification tree.

```{r}


```

### Variable Importance and Plot

We can inspect the tree results by plotting the pruned classification tree. However, be prepared that also pruned trees can be quite large.

```{r}


```

The `varImp()` function is useful for listing the importance of each predictor variable for reducing node impurity.

```{r}


```

## Prediction

For evaluating performance, we predict the outcome in the test set in two formats. We want to use `predict()` for predicting class membership and also for computing predicted probabilities. Therefore, two prediction objects are generated.

```{r}
y_class <- predict(..., newdata = bach_test, type = "class")
y_prob <- predict(..., newdata = bach_test, type = "prob")
head(y_prob)
```

Given predicted class membership, we can use the function `confusionMatrix()` for evaluating our classification model.

```{r}
confusionMatrix(y_class, bach_test$D_loss, mode = "everything", positive = "loss")
```

Additionally, ROC curves are helpful for evaluating prediction performance with categorical outcomes. Here we could (e.g.) use the `pROC` package, which has a function called `roc()`.

```{r}


```

On this basis, we can print and plot the resulting roc object.

```{r}


```

Extra task: Try to calculate precision at top 100, i.e. the expected precision when classifying the 100 test observations with the highest risk scores as `loss`. For this, we need to create a new prediction vector. The function `order()` might be helpful here.

```{r}


```

Finally, compute the `precision()` given the new predicted classes.

```{r}


```

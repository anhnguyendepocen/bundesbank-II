---
title: "Big Data Analysis"
subtitle: "Bagging, Random Forests and Boosting"
author: "Christoph Kern"
output: html_notebook
---

## Setup

```{r}
library(caret)
library(randomForest)
library(xgboost)
library(pROC)
library(ranger)
```

In this exercise, we again use the reduced version of the BACH data set.

```{r}
load("BACH.Rda")
```

## Data preparation

Our machine learning task is -- again -- to predict whether a corporation experiences a net loss in the year 2015. Therefore, we first compute a binary variable which indicates a loss with "loss" and returns "no_loss" otherwise, based on the variable `bach$net_profit_or_loss`.

```{r}
bach$D_loss <- ifelse(bach$net_profit_or_loss < 0, "loss", "no_loss")
bach$D_loss <- as.factor(bach$D_loss)
summary(bach$D_loss)
```

Then we split the data set into a training and test part, using the year 2015 for the test set.

```{r}


```

## Random Forest

We may want to grow a random forest as a first classifier, using `caret`. This time, we want to account for the longitudinal data structure in the cross-validation process. For this, take a look at the function `groupKFold()` and apply it as needed here.

```{r}


```

The resulting object from `groupKFold()` can be passed to the `index` argument of the `trainControl()` function. Furthermore, we use cross-validation as the evaluation method for model tuning.

```{r}
ctrl  <- trainControl(method = "cv",
                      index = ...,
                      summaryFunction = twoClassSummary,
                      classProbs = TRUE,
                      verboseIter = TRUE)
```

Before training random forests, we specifiy try-out values for the tuning parameter(s).

```{r}


```

Now we can use `train()` from `caret` in order to grow the forest. Use the binary loss variable as the outcome and `~ . - net_profit_or_loss - return_on_equity` on the right hand side of the function call.

```{r}


```

Here we can add some code for inspecting the random forest results.

```{r}


```

### Boosting

We may want to use Boosting as an additional prediction method. When using `xgboost`, it is useful to specify a tuning grid first.

```{r}


```

Now we can pass this grid to `train()`, using `xgbTree` as the machine learning method. Many arguments can be copied from the previous call to `train()`.

```{r}


```

Again, take a look at the results from the tuning process, e.g. by printing and/or plotting the corresponding object.

```{r}


```

## Prediction

Next, we can use `predict()` in order to predict class membership and predicted probabilities in the test set based on the results from both classifiers.

```{r}


```

Given predicted class membership, we can use `confusionMatrix()` for evaluating prediction performance.

```{r}


```

Finally, we add ROC curves for comparing the random forest and XGBoost results.

```{r}


```

---
title: "Big Data Analysis"
subtitle: "Bagging, Random Forests and Boosting"
author: "Christoph Kern"
output: html_notebook
---

## Setup

```{r}
library(caret)
library(randomForest)
library(xgboost)
library(pROC)
library(ranger)
```

In this exercise, we again use the reduced version of the BACH data set.

```{r}
load("BACH.Rda")
```

## Data preparation

Our machine learning task is -- again -- to predict whether a corporation experiences a net loss in the year 2015. Therefore, we first compute a binary variable which indicates a loss with "loss" and returns "no_loss" otherwise, based on the variable `bach$net_profit_or_loss`.

```{r}
bach$D_loss <- ifelse(bach$net_profit_or_loss < 0, "loss", "no_loss")
bach$D_loss <- as.factor(bach$D_loss)
summary(bach$D_loss)
```

Then we split the data set into a training and test part, using the year 2015 for the test set.

```{r}
bach_test <- bach[bach$year == "2015",]
bach_train <- bach[bach$year != "2015",]
```

## Random Forest

We may want to grow a random forest as a first classifier, using `caret`. This time, we want to account for the longitudinal data structure in the cross-validation process. For this, take a look at the function `groupKFold()` and apply it as needed here.

```{r}
folds <- groupKFold(bach_train$year)
lengths(folds)
```

The resulting object from `groupKFold()` can be passed to the `index` argument of the `trainControl()` function. Furthermore, we use cross-validation as the evaluation method for model tuning.

```{r}
ctrl  <- trainControl(method = "cv",
                      index = folds,
                      summaryFunction = twoClassSummary,
                      classProbs = TRUE,
                      verboseIter = TRUE)
```

Before training random forests, we specifiy try-out values for the tuning parameter(s).

```{r}
ncols <- ncol(model.matrix(D_loss ~ . - net_profit_or_loss - return_on_equity, 
                           data = bach_train)[,-1])

grid <- expand.grid(mtry = c(floor(log(ncols)), floor(sqrt(ncols))),
                    splitrule = "gini",
                    min.node.size = 5)
```

Now we can use `train()` from `caret` in order to grow the forest. Use the binary loss variable as the outcome and `~ . - net_profit_or_loss - return_on_equity` on the right hand side of the function call.

```{r}
rf <- train(D_loss ~ . - net_profit_or_loss - return_on_equity,
            data = bach_train,
            method = "ranger",
            trControl = ctrl,
            tuneGrid = grid,
            importance = 'impurity',
            metric = "ROC")
```

Here we can add some code for inspecting the random forest results.

```{r}
rf
varImp(rf)
```

### Boosting

We may want to use Boosting as an additional prediction method. When using `xgboost`, it is useful to specify a tuning grid first.

```{r}
grid <- expand.grid(max_depth = 1:3,
                    nrounds = c(250, 500),
                    eta = 0.05,
                    min_child_weight = 5,
                    subsample = 0.7,
                    gamma = 0,
                    colsample_bytree = 1)

grid
```

Now we can pass this grid to `train()`, using `xgbTree` as the machine learning method. Many arguments can be copied from the previous call to `train()`.

```{r}
xgb <- train(D_loss ~ . - net_profit_or_loss - return_on_equity,
             data = bach_train,
             method = "xgbTree",
             trControl = ctrl,
             tuneGrid = grid,
             metric = "ROC")
```

Again, take a look at the results from the tuning process, e.g. by printing and/or plotting the corresponding object.

```{r}
xgb
plot(xgb)
varImp(xgb)
```

## Prediction

Next, we can use `predict()` in order to predict class membership and predicted probabilities in the test set based on the results from both classifiers.

```{r}
rf_class <- predict(rf, newdata = bach_test)
xgb_class <- predict(xgb, newdata = bach_test)

rf_prob <- predict(rf, newdata = bach_test, type = "prob")
xgb_prob <- predict(xgb, newdata = bach_test, type = "prob")
```

Given predicted class membership, we can use `confusionMatrix()` for evaluating prediction performance.

```{r}
confusionMatrix(rf_class, bach_test$D_loss)
confusionMatrix(xgb_class, bach_test$D_loss)
```

Finally, we add ROC curves for comparing the random forest and XGBoost results.

```{r}
roc.list <- roc(bach_test$D_loss ~ rf_prob$loss + xgb_prob$loss)
ggroc(roc.list)
```
